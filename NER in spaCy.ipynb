{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition in spaCy\n",
    "Kate Riesbeck  \n",
    "19 May 2020  \n",
    "  \n",
    "   \n",
    "This notebook reviews named entity recognition (NER) in spaCy with:\n",
    "* a pretrained spaCy model\n",
    "* spaCy lookup\n",
    "* a custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "pip install requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"George Washington (February 22, 1732[b] – December 14, 1799) was an American political leader, military general, statesman, and founding father who served as the first president of the United States from 1789 to 1797. Previously, he led Patriot forces to victory in the nation's War for Independence. He presided at the Constitutional Convention of 1787, which established the U.S. Constitution and a federal government. Washington has been called the \"Father of His Country\" for his manifold leadership in the formative days of the new nation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington 0 17 PERSON\n",
      "February 22 19 30 DATE\n",
      "December 14, 1799 42 59 DATE\n",
      "American 68 76 NORP\n",
      "first 162 167 ORDINAL\n",
      "the United States 181 198 GPE\n",
      "1789 to 1797 204 216 DATE\n",
      "Patriot 237 244 PERSON\n",
      "War for Independence 279 299 EVENT\n",
      "the Constitutional Convention 316 345 LAW\n",
      "1787 349 353 DATE\n",
      "the U.S. Constitution 373 394 LAW\n",
      "Washington 421 431 GPE\n",
      "the \"Father of His Country\" 448 475 LAW\n",
      "the formative days 507 525 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add entities with spaCy lookup\n",
    "\n",
    "pip install spacy-lookup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spacy-lookup matches on token text (not a statistical prediction)\n",
    "\n",
    "can be used alone or added to a pipeline with an existing model \n",
    "\n",
    "https://github.com/mpuig/spacy-lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_lookup import Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = [\"Donald Trump\" , \"Barack Obama\" , \"George W. Bush\" , \"Bill Clinton\" , \"George H.W. Bush\" , \"Ronald Reagan\" , \"Jimmy Carter\" , \"Gerald Ford\" , \"Richard Nixon\" , \"Lyndon B. Johnson\" , \"John F. Kennedy\" , \"Dwight D. Eisenhower\", \"Harry S. Truman\" , \"Franklin D. Roosevelt\" , \"Herbert Hoover\" , \"Calvin Coolidge\" , \"Warren G. Harding\" , \"Woodrow Wilson\" , \"Howard Taft\" , \"Theodore Roosevelt\" , \"William McKinley\" , \"Grover Cleveland\" , \"Benjamin Harrison\" , \"Grover Cleveland\" , \"Chester A. Arthur\" , \"James Garfield\" , \"Rutherford B. Hayes\" , \"Ulysses S. Grant\" , \"Andrew Johnson\" , \"Abraham Lincoln\" , \"James Buchanan\" , \"Franklin Pierce\" , \"Millard Fillmore\", \"Zachary Taylor\" , \"James K. Polk\" , \"John Tyler\" , \"William Henry Harrison\" , \"Martin Van Buren\" , \"Andrew Jackson\" , \"John Quincy Adams\" , \"James Monroe\" , \"James Madison\" , \"Thomas Jefferson\" , \"John Adams\" , \"George Washington\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new \"entity\" pipeline component\n",
    "\n",
    "# new labels can be added with via a list, dictionary, or file\n",
    "\n",
    "new_entities = Entity(keywords_list=presidents, label='PRES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new entity component before the existing 'ner' pipeline\n",
    "nlp.add_pipe(new_entities, before='ner', name='presidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George H.W. Bush 5 21 PRES\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"When George H.W. Bush was elected.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Bush 5 16 PERSON\n"
     ]
    }
   ],
   "source": [
    "# limitation -- only finds exact matches\n",
    "\n",
    "doc = nlp(u\"When George Bush was elected.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy\n",
    "\n",
    "# spaCy’s models are statistical and every “decision” they make whether a word is a named entity is a prediction. \n",
    "# This prediction is based on the examples the model has seen during training.\n",
    "\n",
    "# The model is then shown the unlabelled text and will make a prediction. \n",
    "# Because we know the correct answer, we can give the model feedback on its prediction in the form of an error gradient of the loss function that calculates the difference between the training example and the expected output. \n",
    "# The greater the difference, the more significant the gradient and the updates to our model.\n",
    "\n",
    "# When training a model, we don’t just want it to memorise our examples — \n",
    "# we want it to come up with theory that can be generalised across other examples. \n",
    "# After all, we don’t just want the model to learn that this one instance of “Amazon” right here is a company — \n",
    "# we want it to learn that “Amazon”, in contexts like this, is most likely a company. \n",
    "# In order to tune the accuracy, we process our training examples in batches, \n",
    "# and experiment with minibatch sizes and dropout rates.\n",
    "\n",
    "# Of course, it’s not enough to only show a model a single example once. \n",
    "# Especially if you only have few examples, you’ll want to train for a number of iterations. \n",
    "# At each iteration, the training data is shuffled to ensure the model doesn’t make any generalisations \n",
    "# based on the order of examples.\n",
    "\n",
    "# Another technique to improve the learning results is to set a dropout rate, \n",
    "# a rate at which to randomly “drop” individual features and representations. \n",
    "# This makes it harder for the model to memorise the training data. \n",
    "# For example, a 0.25dropout means that each feature or internal representation has a 1/4 likelihood of being dropped. \n",
    "# We train the model for 10 epochs and keep the dropout rate as 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results and Evaluation of the model :\n",
    "The model is tested on 20 resumes and the predicted summarized resumes are stored as separate .txt files for each resume.\n",
    "\n",
    "For each resume on which the model is tested, we calculate the accuracy score, precision, recall and f-score for each entity that the model recognizes. The values of these metrics for each entity are summed up and averaged to generate an overall score to evaluate the model on the test data consisting of 20 resumes. The entity wise evaluation results can be observed below . It is observed that the results obtained have been predicted with a commendable accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prodigy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
